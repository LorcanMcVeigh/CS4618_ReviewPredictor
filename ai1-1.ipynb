{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action=\"ignore\", category=DataConversionWarning)\n",
    "warnings.filterwarnings(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"reviews.csv.gz\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48136, 15)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>body type</th>\n",
       "      <th>bust size</th>\n",
       "      <th>category</th>\n",
       "      <th>fit</th>\n",
       "      <th>height</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>rented for</th>\n",
       "      <th>review_date</th>\n",
       "      <th>review_summary</th>\n",
       "      <th>review_text</th>\n",
       "      <th>size</th>\n",
       "      <th>user_id</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>47894.000000</td>\n",
       "      <td>44544</td>\n",
       "      <td>43567</td>\n",
       "      <td>48136</td>\n",
       "      <td>48136</td>\n",
       "      <td>47952</td>\n",
       "      <td>4.813600e+04</td>\n",
       "      <td>48116.000000</td>\n",
       "      <td>48133</td>\n",
       "      <td>48136</td>\n",
       "      <td>48136</td>\n",
       "      <td>48136</td>\n",
       "      <td>48136.000000</td>\n",
       "      <td>48136.000000</td>\n",
       "      <td>40594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>95</td>\n",
       "      <td>62</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>2120</td>\n",
       "      <td>41417</td>\n",
       "      <td>47935</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>hourglass</td>\n",
       "      <td>34b</td>\n",
       "      <td>dress</td>\n",
       "      <td>fit</td>\n",
       "      <td>5' 4\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wedding</td>\n",
       "      <td>June 15, 2016</td>\n",
       "      <td>b'Stylist Review'</td>\n",
       "      <td>b'.'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>130lbs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>13887</td>\n",
       "      <td>6732</td>\n",
       "      <td>23179</td>\n",
       "      <td>35430</td>\n",
       "      <td>7100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14411</td>\n",
       "      <td>208</td>\n",
       "      <td>230</td>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>33.866580</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.043875e+06</td>\n",
       "      <td>9.084255</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.248255</td>\n",
       "      <td>499091.944470</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.018446</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.067556e+05</td>\n",
       "      <td>1.436092</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.523070</td>\n",
       "      <td>288853.095706</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.233730e+05</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.941820e+05</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>249516.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>32.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.458800e+05</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>498811.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.678888e+06</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>749178.750000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>117.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.966087e+06</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>999997.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 age  body type bust size category    fit height  \\\n",
       "count   47894.000000      44544     43567    48136  48136  47952   \n",
       "unique           NaN          7        95       62      3     24   \n",
       "top              NaN  hourglass       34b    dress    fit  5' 4\"   \n",
       "freq             NaN      13887      6732    23179  35430   7100   \n",
       "mean       33.866580        NaN       NaN      NaN    NaN    NaN   \n",
       "std         8.018446        NaN       NaN      NaN    NaN    NaN   \n",
       "min         0.000000        NaN       NaN      NaN    NaN    NaN   \n",
       "25%        29.000000        NaN       NaN      NaN    NaN    NaN   \n",
       "50%        32.000000        NaN       NaN      NaN    NaN    NaN   \n",
       "75%        37.000000        NaN       NaN      NaN    NaN    NaN   \n",
       "max       117.000000        NaN       NaN      NaN    NaN    NaN   \n",
       "\n",
       "             item_id        rating rented for    review_date  \\\n",
       "count   4.813600e+04  48116.000000      48133          48136   \n",
       "unique           NaN           NaN          8           2120   \n",
       "top              NaN           NaN    wedding  June 15, 2016   \n",
       "freq             NaN           NaN      14411            208   \n",
       "mean    1.043875e+06      9.084255        NaN            NaN   \n",
       "std     8.067556e+05      1.436092        NaN            NaN   \n",
       "min     1.233730e+05      2.000000        NaN            NaN   \n",
       "25%     1.941820e+05      8.000000        NaN            NaN   \n",
       "50%     9.458800e+05     10.000000        NaN            NaN   \n",
       "75%     1.678888e+06     10.000000        NaN            NaN   \n",
       "max     2.966087e+06     10.000000        NaN            NaN   \n",
       "\n",
       "           review_summary review_text          size        user_id  weight  \n",
       "count               48136       48136  48136.000000   48136.000000   40594  \n",
       "unique              41417       47935           NaN            NaN     173  \n",
       "top     b'Stylist Review'        b'.'           NaN            NaN  130lbs  \n",
       "freq                  230          18           NaN            NaN    3576  \n",
       "mean                  NaN         NaN     12.248255  499091.944470     NaN  \n",
       "std                   NaN         NaN      8.523070  288853.095706     NaN  \n",
       "min                   NaN         NaN      0.000000      47.000000     NaN  \n",
       "25%                   NaN         NaN      8.000000  249516.000000     NaN  \n",
       "50%                   NaN         NaN     12.000000  498811.000000     NaN  \n",
       "75%                   NaN         NaN     16.000000  749178.750000     NaN  \n",
       "max                   NaN         NaN     58.000000  999997.000000     NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>So from the above I could see that there was 48136 examples in the dataset however many of the columns were missing values </p>\n",
    "<p>Also some possible outliers can be seen, such as the maximum age is 117</p>\n",
    "<p>Also the height and weight which should be numbers are strings/objects</p>\n",
    "<p>The most common review is empty so those are not needed and should be removed</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['weight'], inplace=True) # maybe impute cause 7000 examples gone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['weight'] = df['weight'].str.strip('lbs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['weight'] = df['weight'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>here I am converting the weight to an int. But something I noticed but i didn't include here was that removing the nan's ended up removing about 7000 examples so this is one feature maybe could be imputed</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 36.,  37.,  33.,  31.,  23.,  47.,  29.,  44.,  42.,  32.,  30.,\n",
       "        28.,  54.,  41.,  38.,  25.,  27.,  46.,  48.,  40.,  39.,  63.,\n",
       "        56.,  34.,  26.,  22.,  53.,  43.,  60.,  45.,  59.,  35.,  50.,\n",
       "        21.,  75.,  66.,  24.,  69.,  17.,  57.,  16.,  49.,  20.,  64.,\n",
       "        51.,  52.,  nan,  19.,  87.,  18.,  67.,  70.,  65.,  58.,  55.,\n",
       "        61.,  62.,  72.,   0.,  85.,  15.,   9.,  68., 116.,  76.,   1.,\n",
       "         4.,   2., 117.,   3.,  14.,  91.,   5.,  73.,  99., 112.,  77.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['age'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['age'] <= 99) & (df['age'] >= 10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Removed the exmaples where the ages were outside these values because could be outliers</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['age'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['5\\' 6\"', '5\\' 5\"', '5\\' 3\"', '5\\' 7\"', '5\\' 4\"', '5\\' 1\"',\n",
       "       '5\\' 2\"', '5\\' 8\"', '5\\' 10\"', '5\\' 9\"', '5\\' 11\"', '5\\' 0\"',\n",
       "       '4\\' 11\"', '6\\' 2\"', '6\\' 0\"', nan, '4\\' 10\"', '6\\' 1\"', '4\\' 9\"',\n",
       "       '4\\' 8\"', '4\\' 7\"', '4\\' 6\"', '6\\' 3\"', '6\\' 6\"', '6\\' 4\"'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['height'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['height'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['height'] = df['height'].str.strip(\"\\\"\")\n",
    "df['height'] = df['height'].str.replace(\"'\", '.')\n",
    "df['height'] = df['height'].str.replace(\" \", '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['height'] = df['height'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Converting the height to a float because the height of a person should be an int/float and the 'distance' betweens heights are related and important for example, if the dress is too short or too long because of their height</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['fit', 'large', 'small'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['fit'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['athletic', 'full bust', 'petite', 'hourglass',\n",
       "       'straight & narrow', 'pear', 'apple', nan], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['body type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['body type'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['32ddd/e', '36d', '34b', '34c', '36c', '32b', '36aa', '32c', '36b',\n",
       "       '34dd', '32d', '36a', '34d', nan, '38ddd/e', '38d', '34a', '38c',\n",
       "       '36ddd/e', '34d+', '32a', '34ddd/e', '32d+', '30dd', '34g', '38b',\n",
       "       '40dd', '32dd', '34h', '36d+', '36h', '36dd', '30c', '34aa', '44b',\n",
       "       '38dd', '30d', '32g', '38d+', '34f', '36g', '28dd', '32aa', '32f',\n",
       "       '28a', '30ddd/e', '40d', '30b', '38a', '30a', '44g', '32h', '38f',\n",
       "       '42g', '28c', '42ddd/e', '42d', '42dd', '36f', '40f', '28aa',\n",
       "       '38g', '30f', '28d', '42b', '30g', '40c', '42c', '40ddd/e', '38aa',\n",
       "       '30h', '28ddd/e', '36i', '28b', '44d', '40h', '40g', '44f', '32j',\n",
       "       '38i', '28f', '36j', '30aa', '44c', '44ddd/e', '38j', '38h',\n",
       "       '44dd', '40b'], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['bust size'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['bust size'], inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sweater', 'gown', 'dress', 'sheath', 'maxi', 'romper', 'top',\n",
       "       'mini', 'shirtdress', 'blouse', 'jumpsuit', 'shift', 'pants',\n",
       "       'coat', 'culotte', 'jacket', 'tank', 'culottes', 'skirt', 'tunic',\n",
       "       'blazer', 'sweatshirt', 'down', 'frock', 'vest', 'overalls',\n",
       "       'skirts', 'cape', 'cardigan', 'bomber', 'shirt', 'suit', 'henley',\n",
       "       'hoodie', 'poncho', 'for', 'kimono', 'blouson', 'pullover',\n",
       "       'trousers', 'turtleneck', 'kaftan', 'pant', 't-shirt', 'ballgown',\n",
       "       'knit', 'legging', 'print', 'trench', 'cami', 'leggings', 'duster',\n",
       "       'trouser', 'tee', 'midi', 'peacoat', 'combo', 'skort', 'parka',\n",
       "       'buttondown', 'crewneck'], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'] = df['category'].str.replace(\"culote\", \"culottes\")\n",
    "df['category'] = df['category'].str.replace(\"leggings\", \"leggings\")\n",
    "df['category'] = df['category'].str.replace(\"pant\", \"pants\")\n",
    "df['category'] = df['category'].str.replace(\"skirt\", \"skirts\")\n",
    "df['category'] = df['category'].str.replace(\"trousers\", \"trouser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Correcting some mistakes</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['everyday', 'wedding', 'work', 'other', 'formal affair', 'party',\n",
       "       'date', 'vacation', nan], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['rented for'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['rented for'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8, 20, 12,  1,  4, 15, 16, 32, 24, 13, 11, 14, 58,  9, 35, 17, 28,\n",
       "        5,  7, 45, 39, 29,  3,  0, 25, 21,  2, 26, 36, 40, 51, 57, 19, 48,\n",
       "       33, 54, 42, 27, 46, 10, 38, 22, 52, 23, 43, 49,  6, 37, 34, 18, 30])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['size'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.,  8.,  6.,  4.,  2., nan])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['rating'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['rating'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"rating\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>So I tried out 8 differernt models, 2 with standardscaler and onehotencoding, 2 with imputing the weights, 2 with tfidf, 2 with PCA all as both regressor and classifier to see which would retrun the best values. PCA because many features see irrelevent to me such as the item_id and the rented_for features</p>\n",
    "<p>I used tfidf to see if I could find similarities in the reviews that would help to see a pattern between the other features</p>\n",
    "<p>And imputing obviously because dropping nan's in weight removes about 7000 examples</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features_1 = [\"age\", \"size\", \"user_id\",\"item_id\", \"weight\", \"height\" ]\n",
    "nominal_features_1 = [\"body type\", \"bust size\", \"category\", \"fit\", \"rented for\", \"review_date\"]\n",
    "\n",
    "numeric_features_2 = [\"age\", \"size\", \"user_id\",\"item_id\", \"weight\", \"height\" ]\n",
    "nominal_features_2 = [\"body type\", \"bust size\", \"category\", \"fit\", \"rented for\", \"review_date\", 'review_summary', 'review_text']\n",
    "\n",
    "preprocessor_with_impute = ColumnTransformer([\n",
    "        (\"num\", Pipeline([(\"imputer\", SimpleImputer(missing_values=np.nan, strategy=\"mean\")),\n",
    "                          (\"scaler\", StandardScaler())]), \n",
    "                numeric_features_2),\n",
    "        (\"nom\", Pipeline([(\"imputer\", SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")), \n",
    "                          (\"binarizer\", OneHotEncoder(handle_unknown=\"ignore\"))]), \n",
    "                nominal_features_2)], \n",
    "        remainder=\"drop\")\n",
    "\n",
    "preprocessor_with_tfidf = (ColumnTransformer([        \n",
    "    (\"num\", StandardScaler(), numeric_features_1),\n",
    "    (\"tfidf\", TfidfVectorizer(), 'review_summary'), \n",
    "    (\"tfidf1\",TfidfVectorizer(), 'review_text'), \n",
    "    (\"nom\", OneHotEncoder(handle_unknown='ignore'),nominal_features_1)]))\n",
    "\n",
    "preprocessor_with_PCA = ColumnTransformer([\n",
    "        (\"num\", PCA(n_components=0.9), numeric_features_2),\n",
    "        (\"nom\", OneHotEncoder(handle_unknown=\"ignore\"), nominal_features_2)], \n",
    "        remainder=\"drop\")\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "(\"num\", StandardScaler(), numeric_features_2),\n",
    "(\"nom\", OneHotEncoder(handle_unknown=\"ignore\"), nominal_features_2)],\n",
    "remainder=\"drop\")\n",
    "\n",
    "pipeline_reg = Pipeline([\n",
    "(\"pre\", preprocessor),\n",
    "(\"est\", LinearRegression())])\n",
    "\n",
    "pipeline_reg_PCA = Pipeline([\n",
    "(\"pre\", preprocessor_with_PCA),\n",
    "(\"est\", LinearRegression())])\n",
    "\n",
    "pipeline_reg_impute = Pipeline([\n",
    "(\"pre\", preprocessor_with_impute),\n",
    "(\"est\", LinearRegression())])\n",
    "\n",
    "pipeline_reg_tfidf = Pipeline([\n",
    "(\"pre\", preprocessor_with_tfidf),\n",
    "(\"est\", LinearRegression())])\n",
    "\n",
    "pipeline_class = Pipeline([\n",
    "(\"pre\", preprocessor),\n",
    "(\"est\", LogisticRegression(multi_class=\"multinomial\", solver=\"newton-cg\"))])\n",
    "\n",
    "pipeline_class_PCA = Pipeline([\n",
    "(\"pre\", preprocessor_with_PCA),\n",
    "(\"est\", LogisticRegression(multi_class=\"multinomial\", solver=\"newton-cg\"))])\n",
    "\n",
    "pipeline_class_impute = Pipeline([\n",
    "(\"pre\", preprocessor_with_impute),\n",
    "(\"est\", LogisticRegression(multi_class=\"multinomial\", solver=\"newton-cg\"))])\n",
    "\n",
    "pipeline_class_tfidf = Pipeline([\n",
    "(\"pre\", preprocessor_with_tfidf),\n",
    "(\"est\", LogisticRegression(multi_class=\"multinomial\", solver=\"newton-cg\"))])\n",
    "\n",
    "\n",
    "maj_pipeline_reg = DummyRegressor(strategy = \"mean\")\n",
    "maj_pipeline_class = DummyClassifier(strategy = \"most_frequent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = ShuffleSplit(n_splits=1, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"neg mean : \", cross_val_score(pipeline_reg_tfidf, df, y, scoring=\"neg_mean_absolute_error\", cv=ss))\n",
    "#print(\"classifier : \",cross_val_score(pipeline_class, df, y, scoring=\"accuracy\", cv=ss))\n",
    "print(\"dummy : \", cross_val_score(maj_pipeline_reg, df, y, scoring=\"neg_mean_absolute_error\", cv=ss))\n",
    "#print(\"dummy : \", cross_val_score(maj_pipeline_class, df, y, scoring=\"accuracy\", cv=ss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Other Models</h1>\n",
    "<p>I did other models for example one where I imputed that value of weight as 7000 examples had a 'nan' value</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>These are the results of hte regressor with imputing the values of weight </p>\n",
    "<p>array([-1.0752906 , -1.05740605, -1.08392036, -1.07584011, -1.07038251,\n",
    "       -1.08051338, -1.07397215, -1.09999074, -1.09103645, -1.07517578]</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>And for the Classifier it ended up being worse than the dummy class</p>\n",
    "\n",
    "<p>classifier : [0.6369583 0.64221434 0.64630488 0.64385056 0.6425648 0.64283765 0.64529332 0.63383356 0.63673581 0.6345524 ]</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Another model was with PCA regressor on the features </p>\n",
    "<p>neg mean :  [-1.10998851 -1.0870953  -1.12144093 -1.11844097 -1.10428622 -1.12002683\n",
    " -1.09118082 -1.12969712 -1.12047193 -1.10897752]</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>And running the first set of features on the classifier</p>\n",
    "<p>classifier :  [0.6369583  0.64221434 0.64630488 0.64385056 0.6425648  0.64283765\n",
    " 0.64529332 0.63383356 0.63673581 0.6345524 ]</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
